# MachineLearning_SleepAnalysis
Hello Everyone, 

I'm glad to announce that this is my very first Machine Leaning project and my very first project in general. I have been learning computer science for some time but I was never able to get to building my own projects. I wanted to build this project to show the knowledge I have in the Machine Learning field. I also wanted to connect this project to something that would actually prove beneficial, and for me, as a college student, building something related to sleep interested me.
# The Project
## Exploratory Data Analysis
The project starts off by creating a dataframe with the dataset using the **read_csv** method from Pandas. From there, using certain Pandas methods, I performed Exploratory Data Analysis to better understand the data I was working with. I checked to see if there were any null values, and in this dataset, there weren't any. I also checked the data types of all features in the dataset. This would help me out later in the project.
## Feature Engineering
Through the data types of all of the features, I was able to plot all features to better understand the impact of each of them. For all of the categorical features, I plotted a bar graph with the x-axis as the different classes of that specific feature and the y-axis as the count of each class. For all of the numerical features, I built a heatmap to show the correlations between each of the features. The only two features that seem to share some correlation were _Sleep Duration_ and _Quality of Sleep_. Since I was working with a small dataset, I didn't really see any reason to take out one of these features. I did however take out the _Person ID_ feature as it provided no impact to the output. Overall, there wasn't much I had to do since the dataset was smaller and each feature excpet for _Person ID_ proved to be useful. 
## Final Preparation of the data
After finishing the feature engineering step, I had to prepare the data to be able to build a model out of it. I first started off by label encoding all of the categorical features. While a gradient-boosting model like XGBoost is able to build a model around data from features that are not encoded, it is good practice to go ahead and encode them anyways. I also went ahead and one-hot encoded all of the non-binary categorical features. Just like with label encoding, XGBoost is able to build a model out of data from features that haven't been label encoded, but it is good practice. After finishing preparing my categorical features, I went ahead and normalized all my numerical features. I then split up my input features from my output feature. At this point, all my data is in Matrix/array instead of a dataframe. Lastly, split my data up into 80% training data and 20% testing data.
## Building the Model
Before realizing how easy it would be to test multiple model types, I had originally intended on only building a XGBoost model. While building my project, I saw how easy it was to build models and test their accuracy. Unironically, the XGBoost model showed the best accuracy, which is why I went ahead with that model. I originally build a model with all the hyperparameters set to their default values. This showed an accuracy of 92%. I later ran a grid search to see if I could optimize the model by tuning the hyperparameters, but I ended up with a lower accuracy. I realized that since the dataset was very small, the default hyperparameters proved to be a better fit that my hyperparameters through the grid search. I lastly checked the accuracy of this model with a confusion matrix.
